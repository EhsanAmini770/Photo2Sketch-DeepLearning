# Photo to Sketch Conversion using Deep Learning

A deep learning project that converts face photos to artistic sketches using Generative Adversarial Networks (GANs). This implementation uses a Pix2Pix GAN architecture with ResNet-9 Generator and PatchGAN Discriminator.

<div align="center">
<img src="docs/diagram.png" width="600" alt="System Architecture">
</div>

## üéØ Project Overview

This project implements an image-to-image translation system that transforms face photographs into artistic pencil sketches. The model is trained on the CUFS (Chinese University Face Sketch) dataset and achieves high-quality sketch generation with preserved facial features.

### Key Features

- **Pix2Pix GAN Architecture**: ResNet-9 Generator + PatchGAN Discriminator
- **High-Quality Output**: 256x256 pixel sketch generation
- **Real-time Inference**: Fast sketch generation from photos
- **Comprehensive Training Tools**: Monitoring, analysis, and visualization
- **Modular Design**: Easy to extend and modify

## üèóÔ∏è Architecture

### Generator (ResNet-9)
- Encoder-Decoder structure with skip connections
- 9 ResNet blocks for feature transformation
- Instance Normalization for stable training
- Input: 3√ó256√ó256 RGB photo ‚Üí Output: 3√ó256√ó256 sketch

### Discriminator (PatchGAN)
- 70√ó70 patch-level classification
- Convolutional layers with LeakyReLU activation
- MSE Loss (LSGAN approach)
- Evaluates local image patches for realism

## üìä Results

### Training Performance (100 Epochs)
- **Generator Loss Improvement**: 61.4%
- **Discriminator Loss Improvement**: 65.3%
- **Total Training Time**: 0.74 hours
- **Average Epoch Time**: 26.8 seconds

<div align="center">
<img src="docs/training_curves.png" width="600" alt="Training Curves">
</div>

### Sample Results

<div align="center">

**Sample 1: High-quality sketch conversion**
<img src="sample_results/comparisons/1_comparison.png" width="400" alt="Sample 1">

**Sample 2: Preserved facial features**
<img src="sample_results/comparisons/10_comparison.png" width="400" alt="Sample 2">

**Sample 3: Artistic style transfer**
<img src="sample_results/comparisons/100_comparison.png" width="400" alt="Sample 3">

</div>

### Quality Progression During Training

The model shows clear improvement in sketch quality throughout training:

<div align="center">

| Epoch 18 | Epoch 67 | Epoch 90 | Epoch 99 |
|:--------:|:--------:|:--------:|:--------:|
| <img src="results/sketch_001_epoch18.png" width="150" alt="Epoch 18"> | <img src="results/sketch_001_epoch67.png" width="150" alt="Epoch 67"> | <img src="results/sketch_001_epoch90.png" width="150" alt="Epoch 90"> | <img src="results/sketch_001_epoch99.png" width="150" alt="Epoch 99"> |

</div>

## üöÄ Quick Start

### Prerequisites

- Python 3.8+
- CUDA-capable GPU (recommended)
- 4GB+ GPU memory

### Installation

1. **Clone the repository**
```bash
git clone https://github.com/yourusername/photo-to-sketch.git
cd photo-to-sketch
```

2. **Install dependencies**
```bash
pip install -r requirements.txt
```

3. **Download the dataset** (optional, for training)
```bash
python download_dataset.py
```

### Usage

#### Generate Sketch from Photo

```bash
python inference.py \
    --checkpoint checkpoints/pix2pix_cufs_enhanced/checkpoint_epoch_100.pth \
    --input_photo path/to/your/photo.jpg \
    --output_sketch output/sketch.png
```

#### Train Your Own Model

```bash
python train.py
```

#### Generate Sample Results

```bash
python generate_samples.py \
    --checkpoint checkpoints/pix2pix_cufs_enhanced/checkpoint_epoch_100.pth \
    --input_dir dataset/CUFS/test_photos \
    --output_dir sample_results
```

## üìÅ Project Structure

```
photo-to-sketch/
‚îú‚îÄ‚îÄ README.md                 # Project documentation
‚îú‚îÄ‚îÄ requirements.txt          # Python dependencies
‚îú‚îÄ‚îÄ .gitignore               # Git ignore rules
‚îú‚îÄ‚îÄ train.py                 # Main training script
‚îú‚îÄ‚îÄ inference.py             # Single image inference
‚îú‚îÄ‚îÄ networks.py              # GAN architecture definitions
‚îú‚îÄ‚îÄ dataset.py               # Dataset loading utilities
‚îú‚îÄ‚îÄ generate_samples.py      # Batch sample generation
‚îú‚îÄ‚îÄ analyze_training.py      # Training analysis tools
‚îú‚îÄ‚îÄ monitor_training.py      # Real-time training monitor
‚îú‚îÄ‚îÄ visualize_training.py    # Training visualization
‚îú‚îÄ‚îÄ evaluate_checkpoints.py  # Model evaluation
‚îú‚îÄ‚îÄ discriminator_evaluation.py # Discriminator analysis
‚îú‚îÄ‚îÄ download_dataset.py      # Dataset download utility
‚îú‚îÄ‚îÄ checkpoints/             # Trained model weights
‚îÇ   ‚îî‚îÄ‚îÄ pix2pix_cufs_enhanced/
‚îÇ       ‚îú‚îÄ‚îÄ checkpoint_epoch_100.pth
‚îÇ       ‚îî‚îÄ‚îÄ logs/
‚îú‚îÄ‚îÄ results/                 # Training progress images
‚îú‚îÄ‚îÄ sample_results/          # Generated sample outputs
‚îÇ   ‚îú‚îÄ‚îÄ sketches/
‚îÇ   ‚îî‚îÄ‚îÄ comparisons/
‚îî‚îÄ‚îÄ docs/                    # Documentation and reports
    ‚îú‚îÄ‚îÄ PROJE_RAPORU.html
    ‚îú‚îÄ‚îÄ diagram.png
    ‚îú‚îÄ‚îÄ loss_analysis.png
    ‚îî‚îÄ‚îÄ training_curves.png
```

## üîß Advanced Usage

### Training Configuration

Key hyperparameters in `train.py`:

```python
IMG_SIZE = 256        # Input/output image size
BATCH_SIZE = 4        # Batch size for training
NUM_EPOCHS = 100      # Number of training epochs
LR = 2e-4            # Learning rate
LAMBDA_L1 = 100.0    # L1 loss weight
```

### Monitoring Training

Real-time training monitoring:
```bash
python monitor_training.py --log_file checkpoints/pix2pix_cufs_enhanced/logs/training_log.txt
```

### Analyzing Results

Comprehensive training analysis:
```bash
python analyze_training.py --checkpoint_dir checkpoints/pix2pix_cufs_enhanced
```

## üé® Technical Details

### Loss Functions

1. **Generator Loss**: Combination of adversarial and L1 losses
   - Adversarial Loss: Fools the discriminator
   - L1 Loss: Preserves image structure and details

2. **Discriminator Loss**: Binary classification loss
   - Real vs. Fake patch classification
   - MSE loss for stable training

### Data Preprocessing

- Images resized to 256√ó256 pixels
- Normalization to [-1, 1] range
- Data augmentation during training
- Paired photo-sketch alignment

## üìà Performance Metrics

- **Generator Loss**: Measures sketch quality and realism
- **Discriminator Loss**: Measures ability to distinguish real vs. fake
- **L1 Loss**: Measures pixel-wise similarity to ground truth
- **Training Time**: Optimized for efficient GPU utilization

## üî¨ Research Background

This project is based on the Pix2Pix paper:
> Isola, P., et al. (2017). "Image-to-Image Translation with Conditional Adversarial Networks." CVPR.

Key improvements:
- Enhanced training monitoring and visualization
- Comprehensive evaluation metrics
- Modular and extensible codebase
- Real-time inference capabilities

## ü§ù Contributing

Contributions are welcome! Please feel free to submit a Pull Request. For major changes, please open an issue first to discuss what you would like to change.

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üôè Acknowledgments

- CUFS Dataset: Chinese University of Hong Kong
- Pix2Pix Architecture: Berkeley AI Research Lab
- PyTorch Framework: Facebook AI Research

## üìû Contact

**Ehsan Amini**
- Department: Software Engineering
- Email: [u201118076@samsun.edu.tr]

---

*This project was developed as part of a Deep Learning course assignment, demonstrating practical application of GANs for image-to-image translation tasks.*
